---
title: Association Rule Mining (ARM)
type: docs
prev: models/clustering
next: models/
math: true
weight: 3
---

## Overall Goal

Use association rule mining on an augmented social media metric dataset in order to a rich ruleset that uncovers common patterns between social media and song popularity.

## Definition

Association rule mining, or ARM, is an unsupervised machine learning method for uncovering relationships that exist in a dataset. The most commonly taught example is using ARM to identify which items frequently purchased together at a grocery store. For example, buying ham might lead to an increased likelihood of buying cheese, and those two items together might increase the likelihood of buying sandwich bread. It is different from clustering because it is not only grouping like items together, it is also identying relationships that exist between them. 

![ARMLogic](/images/arm/ARMLogic.png)

[Image source](https://algobeans.com/wp-content/uploads/2016/04/association-rules-network-graph2.png)

Additionally, ARM does not use record data like clustering, instead relying on transaction data. To transform a record dataset with quantitative features to a transactional dataset, quantitative values can be binned (e.g., low and high), and those binnings act as the items in a transaction (a row in the dataset). The values of categorical features naturally translate to transactional data (e.g., feature `Genre` has "Pop", then "Pop" becomes an item in the transaction).  

In a rule generated by ARM, the condition of the rule is called the "antecedent", while the outcome that the antecedent implies is called the "consequent". Rules have the form $\text{A}\Rightarrow\text{B}$. Here, $\text{A}$ is an antecedent, while $\text{B}$ is a consequent. This rule suggests that in transactions where $\text{A}$ appears, $\text{B}$ is likely to appear as well. Note that an itemset is a superset of an item. It simply allows for multiple items to be used as antecedents and consequents for a given rule. Three key metrics are used in association rule mining:

1. **Support:** The proportion of transactions with a particular itemset. It represents the most frequently occuring itemsets:

$$\text{Support(A)} = \frac{\text{transactions with A}}{\text{total transactions}}$$

2. **Confidence:** The probability that a transaction with one itemset also contains another itemset. Essentially the amount to which the rule can be depended on:

$$\text{Confidence(A}\Rightarrow\text{B)} = \frac{\text{Support(A}\cup\text{B})}{\text{Support(B)}}$$

3. **Lift:** The confidence of a rule $\text{A}\Rightarrow\text{B}$ over the prevalence of consequent $\text{B}$. It boils down to association strength between the antecedent and the consequent. 

$$\text{Lift(A}\Rightarrow\text{B)} = \frac{\text{Confidence(A}\Rightarrow\text{B})}{\text{Support(B)}}$$

The example image below shows how the output from ARM can be visualized as a network to see how a particular purchase in a grocery store results in other purchases. This example also demonstrates the power of ARM. With these rules, the grocery store could arrange items that are commonly purchased together nearby, which would increase the number of sales and increase the grocery store's profit. 

![ARMExample](/images/arm/ARMExample.png)

[Image source](https://medium.com/@utkarsh.kant/comparing-association-rule-mining-with-other-similar-methods-d964eaafad91)

The most common ARM algorithm is the **Apriori Algorithm**. At a high level, the Apriori algorithm finds relationships between items in large datasets by identifying frequently occurring item combinations. It starts by scanning the data to find individual items that appear frequently enough, then gradually builds larger groups of frequently co-occurring items. At each stage, it prunes combinations that are too infrequent (appearing less than some minimum support), ensuring that only promising patterns are explored. This process continues until no more meaningful itemsets can be found. Finally, the algorithm yields rules, such as "people who buy ham and cheese often buy bread". The utility of each rule can be measured using confidence and lift. 

## Data Preparation

Unlike clustering algorithms, ARM prefers categorical features. Since we want to use ARM on an augmented social media metric dataset to uncover common patterns between social media and song popularity, we will start with our base dataset instead of our PCA-reduced dataset. 

The ARM-Ready dataset contains information about genres (which have been mapped from subgenres to more major genres, e.g., Hard Rock to Rock), whether a track is explicit or not, as well as augmented social media metrics. Originally, only the base social media features, such as `YouTube Views` and `TikTok Likes` were used, but these resulted in boring rulesets. To enrich the output produced by ARM, the following rules were added to the dataset:

```python {filename=""}
# TikTok
df['TikTok Views per Post'] = df['TikTok Views'] / df['TikTok Posts'] #
df['TikTok Impact'] = df['TikTok Views per Post'] / (df['YouTube Views'] + 1) #
df['TikTok View-to-Like Ratio'] = df['TikTok Views'] / (df['TikTok Likes'] + 1) #
df['TikTok Likes per Post'] = df['TikTok Likes'] / (df['TikTok Posts'] + 1) #

# YouTube
df['YouTube View-to-Like Ratio'] = df['YouTube Views'] / (df['YouTube Likes'] + 1) #

# Shazam
df['Shazam Conversion Rate'] = df['Shazam Counts'] / (df['YouTube Views'] + df['TikTok Views'] + 1) #
```

Finally, `Spotify Streams` was used so that rules between social media data and streaming data could be uncovered. To apply the Apriori algorithm in Python, the mlxtend library was used. The thresholds used were a minumum support of 0.15 and a minimum confidence of 0.5.

>[!NOTE]
>Source code can be found here:\
>[github.com/michael-van-vuuren/csci5612-workspace/models/unsupervised/ARM.ipynb](https://github.com/michael-van-vuuren/csci5612-workspace/blob/main/models/unsupervised/ARM.ipynb)

**Starting Data:**
![PreARMReady](/images/arm/PreARMReady.png)

**ARM-Ready Data:**
![ARMReady](/images/arm/ARMReady.png)

>[!NOTE]
>Although the Apriori Algorithm requires transaction data, mlxtend's implementation requires a one-hot encoded dataset. Functionally, they are equivalent.

## Results

Below, the rules uncovered by Apriori are sorted according to support, confidence, and lift metrics. The top 15 rules according to each metric are displayed in a table and network graph. Each node in the graph represents an itemset, and each edge represents a rule. The nodes are colored according to whether they are an antecedent, a consequent, or both. Additionally, the number of displayed along each edge is the value of that rule according to the metric the rules were sorted by. 

When ordered by support, we see rules that expose correlations in the data. For example, high `YouTube View-to-Like Ratio` correlates with high `Spotify Streams`. These rules are not that exciting.

When ordered by confidence, we see many directional rules. For example, when `Genre` is Rap, the data suggests that `Explicit` is Yes. This means that Rap music is more likely to be explicit. We see other interesting rules like when `Spotify Streams` is high and `TikTok Impact` is high, `TikTok Likes per Post` is more likely to be high. 

When ordered by support, we see a mixture of rules. For example, when a song has high spotify streams, but is not explicit and has a low number of likes per post on TikTok, its `TikTok Impact` is low. 

By exploring these different rules, we can identify how social media influences song popularity, in addition to other patterns. 

{{< tabs items="By Support,By Confidence,By Lift" >}}
  {{< tab >}}

  **Rules:**

  ![Top15Support](/images/arm/Top15Support.png)

  **Visualization:**

  ![Top15SupportGraph](/images/arm/Top15SupportGraph.png)
  
  {{< /tab >}}
  {{< tab >}}

  **Rules:**

  ![Top15Confidence](/images/arm/Top15Confidence.png)

  **Visualization:**

  ![Top15ConfidenceGraph](/images/arm/Top15ConfidenceGraph.png)
  
  {{< /tab >}}
  {{< tab >}}

  **Rules:**

  ![Top15Lift](/images/arm/Top15Lift.png)

  **Visualization:**

  ![Top15LiftGraph](/images/arm/Top15LiftGraph.png)
  
  {{< /tab >}}
{{< /tabs >}}

## Conclusions

By applying association rule mining to social media and streaming data, we uncovered key insights into song popularity. The analysis focused on identifying relationships between engagement metrics and overall success.  

One of the most significant findings was that view-to-like ratios matter more than absolute counts—a high ratio indicates strong audience engagement, making it a critical factor in predicting popularity. Additionally, Shazam conversion rates emerged as a valuable metric, as users actively searching for a song often translate into increased streams. Lastly, genre plays a crucial role in social media performance, which in turn affects overall popularity; certain genres thrive on platforms like TikTok and YouTube, leading to viral success and higher streaming numbers.

These insights highlight how different engagement metrics interact, offering a deeper understanding of the factors driving a track’s success. These rules are also useful to consider when deciding how to conduct supervised machine learning methods.
